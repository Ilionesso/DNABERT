{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e8b50-69b8-444e-a182-0d80318d6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup ./Script-A_pretraining-PMI-batchsize-10.sh> output.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc22bd84-a74f-421f-b979-3b70b08e63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07 19:27:17.676493: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "02/07/2025 19:27:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "02/07/2025 19:27:21 - INFO - transformers.configuration_utils -   loading configuration file ./output6/broken/checkpoint-3000/config.json\n",
      "02/07/2025 19:27:21 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 4101\n",
      "}\n",
      "\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "02/07/2025 19:27:21 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /home.nfs/sheikili/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e\n",
      "02/07/2025 19:27:21 - INFO - transformers.modeling_utils -   loading weights file ./output6/broken/checkpoint-3000/pytorch_model.bin\n",
      "/home.nfs/sheikili/DNABERT/src/transformers/modeling_utils.py:651: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "02/07/2025 19:27:22 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/07/2025 19:27:22 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "02/07/2025 19:27:22 - INFO - __main__ -   finish loading model\n",
      "02/07/2025 19:27:22 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='./Data-B_fewshot-task-datasets/Prom-300/10/', model_type='dna', n_process=8, should_continue=False, model_name_or_path='./output6/broken/checkpoint-3000/', task_name='dnaprom', output_dir='./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/', visualize_data_dir=None, result_dir=None, config_name='', tokenizer_name='dna6', cache_dir='', predict_dir=None, max_seq_length=310, do_train=True, do_eval=True, do_predict=False, do_visualize=False, visualize_train=False, do_ensemble_pred=False, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=5, per_gpu_eval_batch_size=9, per_gpu_pred_batch_size=8, early_stop=0, predict_scan_size=1, gradient_accumulation_steps=1, learning_rate=0.0001, weight_decay=0.01, adam_epsilon=1e-08, beta1=0.9, beta2=0.999, max_grad_norm=1.0, attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1, rnn_dropout=0.0, rnn='lstm', num_rnn_layer=2, rnn_hidden=768, num_train_epochs=20.0, max_steps=-1, warmup_steps=0, warmup_percent=0.1, logging_steps=100, save_steps=4000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, visualize_models=None, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), output_mode='classification')\n",
      "02/07/2025 19:27:22 - INFO - __main__ -   Loading features from cached file ./Data-B_fewshot-task-datasets/Prom-300/10/cached_train_checkpoint-3000_310_dnaprom\n",
      "/home.nfs/sheikili/GeneMask/Code-D_run_finetune.py:784: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(cached_features_file)\n",
      "/home.nfs/sheikili/GeneMask/Code-D_run_finetune.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
      "IKA\n",
      "/home.nfs/sheikili/GeneMask/Code-D_run_finetune.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
      "02/07/2025 19:27:23 - INFO - __main__ -   ***** Running training *****\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Num examples = 20\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Num Epochs = 20\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Instantaneous batch size per GPU = 5\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "02/07/2025 19:27:23 - INFO - __main__ -     Total optimization steps = 80\n",
      "Epoch:   0%|                                             | 0/20 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A/home.nfs/sheikili/DNABERT/src/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at /tmp/eb-build/PyTorch/2.4.0/foss-2023b-CUDA-12.4.0/pytorch-v2.4.0/torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.66it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.86it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.91it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.90it/s]\u001B[A\n",
      "Epoch:   5%|█▊                                   | 1/20 [00:02<00:40,  2.11s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  10%|███▋                                 | 2/20 [00:04<00:37,  2.06s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  15%|█████▌                               | 3/20 [00:06<00:34,  2.05s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  20%|███████▍                             | 4/20 [00:08<00:32,  2.04s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  25%|█████████▎                           | 5/20 [00:10<00:30,  2.04s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.96it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.96it/s]\u001B[A\n",
      "Epoch:  30%|███████████                          | 6/20 [00:12<00:28,  2.04s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.96it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.96it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.96it/s]\u001B[A\n",
      "Epoch:  35%|████████████▉                        | 7/20 [00:14<00:26,  2.04s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.98it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  40%|██████████████▊                      | 8/20 [00:16<00:24,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.98it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  45%|████████████████▋                    | 9/20 [00:18<00:22,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.98it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  50%|██████████████████                  | 10/20 [00:20<00:20,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.98it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  55%|███████████████████▊                | 11/20 [00:22<00:18,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.98it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\u001B[A\n",
      "Epoch:  60%|█████████████████████▌              | 12/20 [00:24<00:16,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.98it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  65%|███████████████████████▍            | 13/20 [00:26<00:14,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  70%|█████████████████████████▏          | 14/20 [00:28<00:12,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  75%|███████████████████████████         | 15/20 [00:30<00:10,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  80%|████████████████████████████▊       | 16/20 [00:32<00:08,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  85%|██████████████████████████████▌     | 17/20 [00:34<00:06,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  90%|████████████████████████████████▍   | 18/20 [00:36<00:04,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch:  95%|██████████████████████████████████▏ | 19/20 [00:38<00:02,  2.03s/it]\n",
      "Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:  25%|████████▌                         | 1/4 [00:00<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  50%|█████████████████                 | 2/4 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Iteration:  75%|█████████████████████████▌        | 3/4 [00:01<00:00,  1.97it/s]\u001B[A\n",
      "Iteration: 100%|██████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\u001B[A\n",
      "Epoch: 100%|████████████████████████████████████| 20/20 [00:40<00:00,  2.03s/it]\n",
      "02/07/2025 19:28:04 - INFO - __main__ -    global_step = 80, average loss = 0.7514003973454237\n",
      "02/07/2025 19:28:04 - INFO - __main__ -   Saving model checkpoint to ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/\n",
      "02/07/2025 19:28:04 - INFO - transformers.configuration_utils -   Configuration saved in ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/config.json\n",
      "02/07/2025 19:28:04 - INFO - transformers.modeling_utils -   Model weights saved in ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/pytorch_model.bin\n",
      "02/07/2025 19:28:05 - INFO - transformers.configuration_utils -   loading configuration file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/config.json\n",
      "02/07/2025 19:28:05 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 4101\n",
      "}\n",
      "\n",
      "02/07/2025 19:28:05 - INFO - transformers.modeling_utils -   loading weights file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/pytorch_model.bin\n",
      "/home.nfs/sheikili/DNABERT/src/transformers/modeling_utils.py:651: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   Model name './Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   Didn't find file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/added_tokens.json. We won't load it.\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/vocab.txt\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/special_tokens_map.json\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/tokenizer_config.json\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   Model name './Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming './Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   Didn't find file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/added_tokens.json. We won't load it.\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/vocab.txt\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/special_tokens_map.json\n",
      "02/07/2025 19:28:06 - INFO - transformers.tokenization_utils -   loading file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/tokenizer_config.json\n",
      "02/07/2025 19:28:06 - INFO - __main__ -   Evaluate the following checkpoints: ['./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/']\n",
      "02/07/2025 19:28:06 - INFO - transformers.configuration_utils -   loading configuration file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/config.json\n",
      "02/07/2025 19:28:06 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 4101\n",
      "}\n",
      "\n",
      "02/07/2025 19:28:06 - INFO - transformers.modeling_utils -   loading weights file ./Data-B_fewshot-task-datasets/Prom-300/10/pmi10000-full-warmup/pytorch_model.bin\n",
      "02/07/2025 19:28:07 - INFO - __main__ -   Creating features from dataset file at ./Data-B_fewshot-task-datasets/Prom-300/10/\n",
      "finish loading examples\n",
      "1 processor started !\n",
      "2 processor started !\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   Writing example 0/500\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 3588 2049 4088 4052 3908 3329 1016 4052 3907 3327 1008 4019 3775 2799 2991 3757 2727 2702 2604 2211 637 2536 1940 3650 2300 996 3972 3587 2047 4078 4012 3746 2681 2520 1875 3389 1256 915 3646 2284 931 3709 2535 1935 3630 2220 673 2680 2516 1859 3325 1000 3988 3652 2307 1022 4076 4004 3715 2557 2024 3987 3647 2287 943 3758 2732 2722 2684 2532 1924 3588 2049 4088 4052 3908 3329 1016 4052 3907 3327 1008 4019 3775 2799 2991 3757 2727 2702 2604 2211 637 2536 1940 3650 2300 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 3556 1924 3585 2038 4043 3869 3174 396 1570 2169 470 1867 3357 1128 402 1596 2275 893 3560 1939 3647 2286 938 3740 2657 2423 1487 1838 3243 671 2670 2476 1700 2692 2563 2046 4075 3997 3685 2440 1555 2109 230 906 3611 2142 362 1435 1631 2415 1453 1703 2703 2606 2219 669 2661 2439 1551 2094 171 670 2666 2460 1633 2424 1490 1849 3288 851 3390 1260 929 3701 2501 1798 3082 25 87 333 1320 1172 577 2293 967 3853 3110 140 547 2175 493 1959 3727 2605 2215 653 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 2723 2685 2534 1932 3619 2175 493 1959 3725 2598 2185 536 2131 317 1253 904 3604 2114 252 995 3966 3564 1956 3716 2562 2043 4061 3943 3470 1580 2209 630 2508 1827 3198 489 1941 3655 2317 1062 139 542 2153 405 1605 2311 1037 40 148 580 2308 1027 4095 4077 4008 3732 2626 2300 995 3968 3572 1986 3836 3044 3971 3582 2027 3997 3687 2446 1579 2206 618 2460 1633 2421 1477 1798 3083 31 111 429 1704 2707 2621 2279 910 3626 2202 604 2404 1412 1537 2040 4052 3907 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 1528 2003 3901 3301 904 3604 2115 254 1001 3990 3660 2339 1149 487 1935 3629 2214 649 2583 2125 294 1164 547 2174 491 1950 3692 2465 1656 2514 1852 3300 900 3587 2046 4074 3996 3681 2422 1482 1819 3167 366 1451 1695 2669 2472 1684 2628 2305 1013 4040 3857 3128 212 835 3327 1005 4008 3732 2625 2293 965 3848 3091 61 230 906 3612 2145 374 1484 1826 3196 481 1909 3525 1797 3079 13 40 147 575 2286 938 3738 2651 2399 1391 1454 1706 2716 2659 2431 1517 1958 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 909 3624 2196 580 2307 1021 4071 3982 3628 2212 642 2556 2020 3969 3576 2004 3908 3329 1016 4051 3903 3311 942 3756 2723 2688 2548 1988 3844 3075 4093 4072 3986 3643 2271 878 3499 1693 2661 2440 1556 2115 255 1006 4011 3743 2672 2484 1729 2806 3020 3876 3204 514 2044 4066 3964 3553 1912 3537 1848 3284 835 3325 999 3983 3632 2228 708 2820 3075 4094 4076 4001 3701 2504 1811 3133 232 915 3645 2278 907 3614 2156 420 1668 2562 2042 4058 3932 3425 1400 1492 1857 3317 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   Writing example 0/500\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-501\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 2428 1507 1917 3559 1934 3628 2210 633 2520 1876 3393 1270 972 3874 3194 474 1881 3415 1358 1324 1188 643 2557 2022 3979 3615 2159 430 1708 2723 2687 2542 1962 3739 2654 2409 1431 1615 2351 1197 678 2697 2584 2132 322 1276 996 3971 3581 2024 3986 3641 2264 851 3389 1255 910 3626 2203 607 2416 1459 1725 2790 2954 3612 2146 380 1505 1911 3533 1829 3207 527 2093 165 645 2565 2053 6 12 34 123 479 1903 3502 1708 2724 2689 2550 1993 3863 3150 300 1186 636 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-502\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 3533 1829 3206 524 2084 131 509 2021 3976 3603 2111 238 939 3743 2670 2476 1699 2688 2546 1980 3810 2940 3555 1919 3565 1960 3730 2619 2271 877 3495 1680 2612 2244 771 3069 4071 3982 3628 2210 634 2524 1892 3460 1538 2044 4065 3960 3540 1860 3330 1019 4063 3950 3499 1694 2668 2465 1656 2515 1855 3310 940 3748 2689 2551 1998 3882 3228 611 2429 1510 1932 3619 2173 485 1927 3597 2087 141 552 2196 580 2305 1015 4047 3885 3239 653 2600 2194 572 2275 894 3564 1954 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-503\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 1940 3652 2305 1014 4043 3869 3176 404 1601 2295 976 3891 3264 755 3008 3827 3007 3824 2996 3779 2815 3054 4011 3744 2675 2496 1779 3006 3819 2975 3695 2479 1711 2734 2732 2723 2686 2540 1956 3715 2559 2032 4019 3776 2803 3007 3823 2991 3760 2739 2752 2803 3006 3819 2975 3695 2479 1712 2739 2752 2803 3006 3819 2975 3695 2478 1708 2724 2691 2558 2026 3995 3679 2415 1455 1710 2731 2719 2671 2477 1704 2707 2623 2288 947 3775 2798 2985 3734 2636 2340 1156 513 2040 4052 3906 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-504\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 3880 3218 570 2266 860 3426 1404 1506 1915 3549 1893 3464 1555 2109 231 909 3623 2189 552 2194 571 2270 875 3486 1642 2458 1628 2403 1408 1523 1983 3823 2990 3755 2718 2665 2455 1614 2348 1186 634 2523 1885 3432 1426 1594 2265 854 3401 1304 1107 317 1255 910 3628 2210 635 2527 1904 3506 1724 2788 2947 3583 2030 4012 3748 2690 2554 2010 3932 3426 1402 1497 1877 3397 1286 1034 27 93 359 1424 1586 2233 727 2896 3378 1210 729 2901 3400 1300 1090 249 981 3912 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   guid: dev-505\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   input_ids: 2 471 1869 3367 1165 551 2189 551 2190 555 2205 615 2445 1574 2188 547 2173 487 1933 3623 2189 551 2190 555 2206 619 2461 1639 2445 1574 2187 543 2157 423 1677 2599 2190 555 2205 615 2445 1575 2190 555 2205 615 2445 1574 2188 547 2173 487 1933 3623 2189 551 2191 559 2224 691 2751 2798 2986 3739 2653 2408 1427 1597 2279 911 3629 2216 658 2619 2270 876 3489 1653 2504 1812 3137 247 975 3885 3240 659 2622 2284 929 3703 2511 1838 3243 669 2663 2447 1583 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "02/07/2025 19:28:08 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "02/07/2025 19:28:08 - INFO - __main__ -   Saving features into cached file ./Data-B_fewshot-task-datasets/Prom-300/10/cached_dev_checkpoint-3000_310_dnaprom\n",
      "02/07/2025 19:28:08 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "02/07/2025 19:28:08 - INFO - __main__ -     Num examples = 1000\n",
      "02/07/2025 19:28:08 - INFO - __main__ -     Batch size = 9\n",
      "Evaluating: 100%|█████████████████████████████| 112/112 [00:30<00:00,  3.70it/s]\n",
      "/home.nfs/sheikili/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "02/07/2025 19:28:38 - INFO - __main__ -   ***** Eval results  *****\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     acc = 0.5\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     auc = 0.5508859999999999\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     f1 = 0.3333333333333333\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     mcc = 0.0\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     precision = 0.25\n",
      "02/07/2025 19:28:38 - INFO - __main__ -     recall = 0.5\n"
     ]
    }
   ],
   "source": [
    "! ./Script-B_finetuning_and_prediction-PMI-batchsize-10.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d422c6a6-8a81-4a0a-b569-4a14499f0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.449\n",
    "# 0.4\n",
    "# 0.445\n",
    "# 0.407\n",
    "# 0.411\n",
    "# 0.398\n",
    "# 0.469\n",
    "# 0.469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999301b7-c057-4877-b787-650b94e0e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'weight_decay': 0.01, 'lr': 0.0005894736842105263, 'betas': (0.9, 0.98), 'eps': 1e-06, 'correct_bias': True, 'initial_lr': 0.0008, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]}, {'weight_decay': 0.0, 'lr': 0.0005894736842105263, 'betas': (0.9, 0.98), 'eps': 1e-06, 'correct_bias': True, 'initial_lr': 0.0008, 'params': [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203]}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
